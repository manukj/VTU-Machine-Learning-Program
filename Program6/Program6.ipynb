{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Program6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manukj/Machine-Learning/blob/master/Program6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVI2Oyrs9xLh",
        "colab_type": "text"
      },
      "source": [
        "#Assuming a set of documents that need to be classified, use the na√Øve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. Calculate the accuracy, precision, and recall for your data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0rl4iZM94vf",
        "colab_type": "text"
      },
      "source": [
        "packages needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfsI_Jl9ZRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65lOH5q29_20",
        "colab_type": "text"
      },
      "source": [
        "for a perticular category, get the train and test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYB-7Gd0_HIl",
        "colab_type": "text"
      },
      "source": [
        "# get the data ready for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYab35F896-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "b27ba28c-9b22-4828-965e-7037b27bba57"
      },
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train',categories=categories,shuffle=True)\n",
        "twenty_test = fetch_20newsgroups(subset='test',categories=categories,shuffle=True)\n",
        "print(\"len(twenty_train.data)=\",len(twenty_train.data))\n",
        "print(\"len(twenty_test.data)=\",len(twenty_test.data))\n",
        "print(\"the train set target names which are based on the categories we have defined=\",twenty_train.target_names)\n",
        "print(\"one taining data instance = \")\n",
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))\n",
        "print(\"output Target for the taining set are = \",twenty_train.target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(twenty_train.data)= 2257\n",
            "len(twenty_test.data)= 1502\n",
            "the train set target names which are based on the categories we have defined= ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
            "one taining data instance = \n",
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "Organization: The City University\n",
            "Lines: 14\n",
            "\n",
            "Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do the same, converting to HPGL (HP plotter) files.\n",
            "\n",
            "Please email any response.\n",
            "\n",
            "Is this the correct group?\n",
            "\n",
            "Thanks in advance.  Michael.\n",
            "-- \n",
            "Michael Collier (Programmer)                 The Computer Unit,\n",
            "Email: M.P.Collier@uk.ac.city                The City University,\n",
            "Tel: 071 477-8000 x3769                      London,\n",
            "Fax: 071 477-8565                            EC1V 0HB.\n",
            "\n",
            "Target [1 1 3 ... 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqPMuPKz_aGk",
        "colab_type": "text"
      },
      "source": [
        "normalize the data, so that it can we sutable for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjdOWrgh_gv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_tf = count_vect.fit_transform(twenty_train.data)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO4LyPT__L6J",
        "colab_type": "text"
      },
      "source": [
        "#create the model, in our case we are using an API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2btK9Gw_Dbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "mod = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2VxpMXu_url",
        "colab_type": "text"
      },
      "source": [
        "# not fit the data to the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro8YW8CY_uES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab625da8-a780-4fd2-d72f-ab91242708de"
      },
      "source": [
        "mod.fit(X_train_tfidf, twenty_train.target)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMJTKnpj_y9R",
        "colab_type": "text"
      },
      "source": [
        "normalize the test data, so it can be tested with the model we have created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amzp9mwp_7H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_tf = count_vect.transform(twenty_test.data)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nt7P5e-_9wO",
        "colab_type": "text"
      },
      "source": [
        "#test the model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cggn2n6sAAjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "5a2235c8-14f7-41f2-856a-59d8f6898467"
      },
      "source": [
        "predicted = mod.predict(X_test_tfidf)\n",
        "print(\"Predicted\",predicted)\n",
        "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))\n",
        "print(classification_report(twenty_test.target,predicted,target_names=twenty_test.target_names))\n",
        "print(\"confusion matrix is \\n\",metrics.confusion_matrix(twenty_test.target, predicted))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted [2 2 3 ... 2 2 1]\n",
            "Accuracy: 0.8348868175765646\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.97      0.60      0.74       319\n",
            "         comp.graphics       0.96      0.89      0.92       389\n",
            "               sci.med       0.97      0.81      0.88       396\n",
            "soc.religion.christian       0.65      0.99      0.78       398\n",
            "\n",
            "              accuracy                           0.83      1502\n",
            "             macro avg       0.89      0.82      0.83      1502\n",
            "          weighted avg       0.88      0.83      0.84      1502\n",
            "\n",
            "confusion matrix is \n",
            " [[192   2   6 119]\n",
            " [  2 347   4  36]\n",
            " [  2  11 322  61]\n",
            " [  2   2   1 393]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}